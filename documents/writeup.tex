%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}
%% NOTE that a single column version may be required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2021}
\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
  June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
\title{Affect Analysis for Cloud Robotics}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Charlie Maclean}
\affiliation{%
  \institution{University of Cambridge}
  \country{United Kingdom}
  \city{Cambridge}
}
\email{cm927@cam.ac.uk}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Charlie Maclean}

\begin{abstract}

As social robots become widespread, it will be essential that they can
classify the emotions of humans around them, in order to interact in a
meaningful and helpful way. But limited hardware means they
may have to offload video data to the cloud (cloud robotics), reducing the resolution of the
content. This work focusses on evaluating the fitness of neural network models
for cloud computing, specifically the effect of changing spatial and temporal 
resolution on the
classification of emotion in video. We build different models to asses each
models performance when given lower resolution video. The results show that by
applying a CNN-LSTM model to a TODO 7-class problem, we can achieve 73\% accuracy on high resolution
video, and maintain 66\% accuracy with the lowest resolutions. To the best of
our knowledge this is the first work that investigates the effect of changing
both spatial and temporal resolution on video-based sentiment classification.

\end{abstract}


\keywords{Affective computing, robotics, cloud computing, emotions, arousal, valence, resolution.}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

\begin{figure}
  \includegraphics[width=\linewidth]{care-robot.jpg}
  \caption{Pepper by SoftBank Robotics, an example of a social robotic
  platform which has been trialled in care homes in the UK. Photograph by Dick
  Thomas, via creative commons.
  (\href{https://search.creativecommons.org/photos/e802da62-8b2f-4f10-9520-bb6c5a0e03db}{source})}
  \label{fig:pepper}
\end{figure}

Social robots are becoming increasingly widespread, with uses in a wide range
of locations, providing help in hospitals \cite{moxi}, care homes \cite{ElliQ}
and schools \cite{AV1}. These robots are frequently required to
interact meaningfully with humans, and in order to do so it is essential that
they are able to classify emotions to react accordingly. However, many social robotic
platforms lack the computational hardware required to perform classification
\cite{celiktutan18}.
Hence, it will likely become necessary to move to a cloud robotic framework,
where sensing data is offloaded to the cloud and processed there. Unreliable
network conditions mean we must be prepared for video data to enter the cloud
at reduced spatial and temporal resolutions. 

In past years, neural networks have become ubiquitous for classifying emotion,
due to their ability to learn pattern humans would be unable to program in.
However they can suffer from not being generalizable, especially if a network
is trained in one domain, then deployed in another. For example, a network
trained on high resolution data would be less effective at classifying low
resolution data. 

For classifying images, there is a large volume of work looking at using
convolutional neural networks (CNNs). When applied to an image, a CNN
convolves a filter with the pixel data, generating meaningful features. 
CNNs have found widespread use across various domains, including facial
recognition \cite{lawrence1997face} and object detection and classification
\cite{szegedy2014going}.
Several architectures have been proposed offering impressive ability to learn
features from video using 
purposes, for example the VGG16 network \cite{simonyan2014very} and the
ResNet50 network \cite{he2016deep}, both of which were able to achieve winning results in
the ImageNet object detection and classification challenge \cite{ILSVRC15}.

For classifying videos, it is often vital to take temporal data into account,
and as a result Recurrent Neural Networks (RNNs) \cite{rumelhart1985learning} are a good
choice. RNNs have some internal state, or memory, which they use to process
sequences, learning patterns that may vary over time. A very popular
architecture is Long Short Term Memory \cite{hochreiter1997long} {(LSTM), which make use of gates to
control the flow into and out of cells in the architecture. LSTMs have found
wide usage, across speech recognition \cite{graves2005framewise}, market prediction
\cite{islam2020foreign} and handwriting recognition \cite{graves2008unconstrained}.

In this work we create a couple of classifiers which are tested on video at a
variety of resolutions and frame-rates, in order to deduce which classifiers
may be useful for cloud robotics. The classifiers are tested on a 7-class
video dataset, and our results show that we can achieve an accuracy of X on

The rest of the paper is structured as follows: Section 2 gives an overview of
previous work on similar problems, Section 3 gives detail about the
methodology employed in the study. Then, Section 4 discusses the results
before Section 5 goes over future research directions. Finally, Section 6
concludes the paper with an overview of the findings. 

\section{Related Work}

\section{Facial Expression Recognition}
\subsection{Images}



\section{Methodology}

\subsection{Transfer Learning}

In order to efficiently train on 

\section{Future Work}
\section{Conclusion}


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{writeup}

%%
%% If your work has an appendix, this is the place to put it.

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
