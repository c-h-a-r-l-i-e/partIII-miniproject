{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quick-violin",
   "metadata": {},
   "source": [
    "# CNN-LSTM Video Sentiment Analysis\n",
    "This notebook will walk through the process of training a LSTM on top of a CNN.\n",
    "\n",
    "This is an early version of the LSTM code, which was updated in server_code/classify_video.py, as it could run quicker there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Input, AveragePooling2D, TimeDistributed, LSTM\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import utils.data\n",
    "import utils.model\n",
    "import gc\n",
    "import pickle\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-event",
   "metadata": {},
   "source": [
    "Set the parameters for our video training procedure. The CNN model is later placed before a LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fer_label_bin', 'rb') as file:\n",
    "    lb = pickle.load(file)\n",
    "dataset = \"fer+\"\n",
    "cnn_model = \"../models/best-models/resnet50-vgg.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-health",
   "metadata": {},
   "source": [
    "Load the data, and setup various functions which help us to use the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "vidlist = glob.glob(\"../data/dataset/{}/*-30-720.mp4\".format(dataset))\n",
    "\n",
    "emotion_dict = {\n",
    "        1: \"neutral\",\n",
    "        2: \"calm\",\n",
    "        3: \"happy\",\n",
    "        4: \"sad\",\n",
    "        5: \"angry\",\n",
    "        6: \"fearful\",\n",
    "        7: \"disgust\",\n",
    "        8: \"surprised\"\n",
    "    }\n",
    "\n",
    "def get_frame_array(filename):\n",
    "    frames = []\n",
    "\n",
    "    vs = cv2.VideoCapture(filename)\n",
    "\n",
    "    # Loop over video frames\n",
    "    while True:\n",
    "        (grabbed, frame) = vs.read()\n",
    "        if not grabbed:\n",
    "            break\n",
    "\n",
    "        # convert to greyscale\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "            \n",
    "        frame = cv2.resize(frame, (197, 197)).astype(\"float32\")\n",
    "        frame -= 128.8006\n",
    "        frame /= 64.6497\n",
    "        frames.append(frame)\n",
    "        \n",
    "    return np.array(frames)\n",
    "        \n",
    "x = []\n",
    "y = []\n",
    "num_frames = 0    \n",
    "for i, video in enumerate(vidlist):\n",
    "    emotion = emotion_dict[int(re.search('(?<=\\/0)\\d', video).group())]\n",
    "    if emotion in lb.classes_:\n",
    "        x.append(video)\n",
    "        y.append(emotion)\n",
    "        num_frames += len(get_frame_array(video))\n",
    "print(num_frames)\n",
    "\n",
    "y = lb.transform(y)\n",
    "\n",
    "# split into test/train/validate\n",
    "train_x , test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "train_x , val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.25, random_state=42)\n",
    "\n",
    "def train_gen(batch_size, num_frames):\n",
    "    while True:\n",
    "        vid_num = 0\n",
    "        vid_index = 0\n",
    "        frames = get_frame_array(train_x[vid_num])\n",
    "        y = train_y[vid_num]\n",
    "        \n",
    "        frames_out = []\n",
    "        classes_out = []\n",
    "        \n",
    "        for batch_num in range(batch_size):\n",
    "            if vid_index + num_frames > len(frames):\n",
    "                vid_num += 1\n",
    "                vid_index = 0\n",
    "                frames = get_frame_array(train_x[vid_num])\n",
    "                y = train_y[vid_num]\n",
    "                \n",
    "                \n",
    "            frames_out.append(frames[vid_index: vid_index + num_frames])\n",
    "            classes_out.append(y)\n",
    "            vid_index += 1\n",
    "            \n",
    "        yield(np.array(frames_out), np.array(classes_out))\n",
    "            \n",
    "            \n",
    "def val_gen(batch_size, num_frames):\n",
    "    while True:\n",
    "        vid_num = 0\n",
    "        vid_index = 0\n",
    "        frames = get_frame_array(val_x[vid_num])\n",
    "        y = val_y[vid_num]\n",
    "        \n",
    "        frames_out = []\n",
    "        classes_out = []\n",
    "        \n",
    "        for batch_num in range(batch_size):\n",
    "            if vid_index + num_frames > len(frames):\n",
    "                vid_num += 1\n",
    "                vid_index = 0\n",
    "                frames = get_frame_array(val_x[vid_num])\n",
    "                y = val_y[vid_num]\n",
    "                \n",
    "                \n",
    "            frames_out.append(frames[vid_index: vid_index + num_frames])\n",
    "            classes_out.append(y)\n",
    "            vid_index += 1\n",
    "            \n",
    "        yield(np.array(frames_out), np.array(classes_out))\n",
    "            \n",
    "def test_gen(batch_size):\n",
    "    assert batch_size == 1\n",
    "    while True:\n",
    "        for i, video_fn in enumerate(test_x):\n",
    "            yield (get_frame_array(video_fn), test_y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-yellow",
   "metadata": {},
   "source": [
    "Load the CNN model which we have previously saved, and place a LSTM after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_frames):\n",
    "    if cnn_model == \"../models/best-models/resnet50-vgg.h5\":\n",
    "        base_model = load_model(\"../models/best-models/resnet50-vgg.h5\")\n",
    "\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # prune the top two layers from the model\n",
    "        base_model._layers.pop()\n",
    "        base_model._layers.pop()\n",
    "        \n",
    "        base_model = Model(inputs=base_model.input, outputs=base_model.layers[-1].output)\n",
    "\n",
    "        base_model = TimeDistributed(base_model, input_shape = [num_frames, 197, 197, 3]) # enable the CNN to be called for each frame\n",
    "\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            LSTM(128),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(len(lb.classes_))\n",
    "        ])\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Model {} not supported yet, please implement that\".format(cnn_model))\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-disposition",
   "metadata": {},
   "source": [
    "Set out our training procedure, using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # default hyperparameters\n",
    "    config_defaults = {\n",
    "        'batch_size' : 1,\n",
    "        'epochs': 30,\n",
    "        'num_frames' : 32\n",
    "    }\n",
    "\n",
    "    wandb.init(project='sentiment', config=config_defaults)\n",
    "    config = wandb.config\n",
    "    \n",
    "    config.architecture_name = \"{} followed by LSTM\".format(cnn_model)\n",
    "    config.dataset = dataset\n",
    "    \n",
    "    # Compile the model\n",
    "    opt = SGD()\n",
    "    model = get_model(config.num_frames)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "        metrics=[\"accuracy\"])\n",
    "\n",
    "    # Now we can start training!\n",
    "    history = model.fit(\n",
    "        train_gen(config.batch_size, config.num_frames),\n",
    "        steps_per_epoch = len(train_x) // config.batch_size,\n",
    "        validation_data = val_gen(config.batch_size, config.num_frames),\n",
    "        validation_steps = len(val_x) // config.batch_size,\n",
    "        epochs = config.epochs,\n",
    "        callbacks = [WandbCallback()]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-hospital",
   "metadata": {},
   "source": [
    "Next, we setup a sweep of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-leone",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_loss\",\n",
    "        \"goal\": \"minimize\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"epochs\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"min\": 20,\n",
    "            \"max\": 40\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"min\": 30,\n",
    "            \"max\": 64\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.00001,\n",
    "            \"max\": 0.001\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.9,\n",
    "            \"max\": 0.99\n",
    "        },\n",
    "        \"decay\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-6,\n",
    "            \"max\": 1e-2\n",
    "        }\n",
    "    },\n",
    "    \"early_terminate\" :\n",
    "    {\n",
    "        \"type\": \"hyperband\",\n",
    "        \"min_iter\": 3\n",
    "    }\n",
    "}\n",
    "wandb.sweep(sweep_config, project='sentiment')\n",
    "wandb.agent(sweep_id, project='sentiment', function=train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
